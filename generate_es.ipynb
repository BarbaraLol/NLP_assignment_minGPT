{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10bba606",
   "metadata": {},
   "source": [
    "# Colab Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "from mingpt.model import GPT\n",
    "from mingpt.bpe import BPETokenizer\n",
    "import torch.nn.functional as F\n",
    "from mingpt.utils import set_seed\n",
    "\n",
    "# Define the clean and corrupted sentences in Spanish\n",
    "FACT_CLEAN = \"El gato brinco rapido desde la mesa hacia la silla cayendo cerca de la ventana grande en la sala\"\n",
    "FACT_CORRUPTED = FACT_CLEAN.replace(\"cayendo\", \"tropezando\")\n",
    "END = \"El gato cayo\"\n",
    "SPECIFIC_TOKENS = [\"c\", \"ay\", \"endo\", \"trope\", \"z\", \"ando\"]\n",
    "\n",
    "def get_specific_token_probs(logits, tokenizer, tokens):\n",
    "    \"\"\"Get probabilities of specific tokens from logits, handling multi-token cases.\"\"\"\n",
    "    probs = F.softmax(logits, dim=-1)  # Apply softmax to logits\n",
    "    token_probs = {}\n",
    "\n",
    "    for token in tokens:\n",
    "        token_ids = tokenizer(token)[0]  # Get token IDs for the word\n",
    "        token_ids = token_ids.tolist() if isinstance(token_ids, th.Tensor) else token_ids\n",
    "\n",
    "        # Sum probabilities for all subword tokens\n",
    "        token_prob = sum(probs[0, token_id].item() for token_id in token_ids)\n",
    "        token_probs[token] = token_prob\n",
    "\n",
    "    return token_probs\n",
    "\n",
    "def generate_heatmap(model_type, diff_matrix, tokens, specific_token):\n",
    "    plt.figure(figsize=(30, 16))\n",
    "    sns.heatmap(diff_matrix,\n",
    "                cmap='crest',\n",
    "                annot=False,\n",
    "                xticklabels=tokens)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.xlabel('Token')\n",
    "    plt.ylabel('Layer')\n",
    "    plt.title(f\"Patching Heatmap of '{specific_token}' Token in the Corrupted Input\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save to a specific directory\n",
    "    output_dir = \"./heatmaps/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "    plt.savefig(f'{output_dir}patching_heatmap_{model_type}_{specific_token}.png')\n",
    "    plt.close()\n",
    "\n",
    "def tokenize_and_print(tokenizer, text, device):\n",
    "    tokens = tokenizer(text).to(device)\n",
    "    tokens_str = [tokenizer.decode(th.tensor([token])) for token in tokens[0]]\n",
    "    print(\"Detokenized input as strings: \" + '/'.join(tokens_str))\n",
    "    return tokens\n",
    "\n",
    "# Pad the shorter sequence to match the longer one\n",
    "def pad_sequences(clean, corrupted):\n",
    "    max_length = max(clean.size(1), corrupted.size(1))\n",
    "    padding = lambda seq, length: th.cat([seq, th.zeros(1, length - seq.size(1), dtype=th.long)], dim=1)\n",
    "    clean_padded = clean if clean.size(1) == max_length else padding(clean, max_length)\n",
    "    corrupted_padded = corrupted if corrupted.size(1) == max_length else padding(corrupted, max_length)\n",
    "    return clean_padded, corrupted_padded\n",
    "\n",
    "# Initialization\n",
    "device = \"mps\" if th.backends.mps.is_available() else \"cuda\" if th.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on device: {device}\")\n",
    "model_type = \"gpt2-medium\"\n",
    "seed = 78\n",
    "set_seed(seed)\n",
    "\n",
    "# Initialize model, tokenizer and input\n",
    "model = GPT.from_pretrained(model_type)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "tokenizer = BPETokenizer()\n",
    "\n",
    "# Tokenize inputs\n",
    "clean = tokenize_and_print(tokenizer, FACT_CLEAN + END, device)\n",
    "corrupted = tokenize_and_print(tokenizer, FACT_CORRUPTED + END, device)\n",
    "\n",
    "# Pad clean and corrupted inputs to ensure equal sequence length\n",
    "clean, corrupted = pad_sequences(clean, corrupted)\n",
    "print(f\"Clean sequence length after padding: {clean.size(1)}\")\n",
    "print(f\"Corrupted sequence length after padding: {corrupted.size(1)}\")\n",
    "\n",
    "# Get predictions for clean input\n",
    "logits_clean, _ = model(clean, store_activations=True)\n",
    "clean_activations = model.layer_activations.copy()\n",
    "print(\"\\nProbabilities for specific tokens in clean input:\")\n",
    "print(get_specific_token_probs(model.last_token_logits, tokenizer, SPECIFIC_TOKENS))\n",
    "reference_logits = model.last_token_logits[0]\n",
    "\n",
    "# Setup patching loop\n",
    "n_layers = len(model.transformer.h)\n",
    "print(f\"Number of layers: {n_layers}\")\n",
    "seq_length = corrupted.size(1)\n",
    "print(f\"Sequence length: {seq_length}\")\n",
    "\n",
    "# Get tokens\n",
    "tokens = [tokenizer.decode(th.tensor([corrupted[0, i].item()])) for i in range(seq_length)]\n",
    "\n",
    "# Iterate through each specific token\n",
    "for specific_token in SPECIFIC_TOKENS:\n",
    "    print(f\"Processing token: {specific_token}\")\n",
    "    diff_matrix = np.zeros((n_layers, seq_length))\n",
    "    \n",
    "    # Get token IDs for clean and corrupted input\n",
    "    token_ids_clean = tokenizer(specific_token)[0]\n",
    "    token_ids_clean = token_ids_clean.tolist() if isinstance(token_ids_clean, th.Tensor) else token_ids_clean\n",
    "\n",
    "    token_ids_corrupted = tokenizer(specific_token.replace(\"rumorosamente\", \"silenziosamente\"))[0]\n",
    "    token_ids_corrupted = token_ids_corrupted.tolist() if isinstance(token_ids_corrupted, th.Tensor) else token_ids_corrupted\n",
    "\n",
    "    # Iterate through layers and positions\n",
    "    for layer in range(n_layers):\n",
    "        for pos in range(seq_length):\n",
    "            # Forward pass with patching\n",
    "            logits_patched, _ = model(\n",
    "                corrupted.to(device),\n",
    "                patch_params=(layer, pos, clean_activations[f'layer_{layer}'][:, pos, :])\n",
    "            )\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            patched_probs = F.softmax(model.last_token_logits[0], dim=-1)\n",
    "            \n",
    "            # Compute probabilities for clean and corrupted\n",
    "            clean_prob = sum(reference_logits[token_id].item() for token_id in token_ids_clean)\n",
    "            patched_prob = sum(patched_probs[token_id].item() for token_id in token_ids_corrupted)\n",
    "\n",
    "            # Compute the probability difference\n",
    "            diff_matrix[layer, pos] = patched_prob - clean_prob\n",
    "\n",
    "    # Generate heatmap\n",
    "    generate_heatmap(model_type, diff_matrix, tokens, specific_token)\n",
    "\n",
    "# Get predictions for corrupted input\n",
    "logits_corrupted, _ = model(corrupted.to(device))\n",
    "print(\"\\nProbabilities for specific tokens in corrupted input:\")\n",
    "print(get_specific_token_probs(logits_corrupted[0], tokenizer, SPECIFIC_TOKENS))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
